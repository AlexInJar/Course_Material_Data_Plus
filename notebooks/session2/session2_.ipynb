{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 2 Scraping\n",
    "- Why is scrapping useful?\n",
    "- Basics of How Browser Works\n",
    "- How to clean from json, \n",
    "- Wikipeadia getting back to Phylisophy\n",
    "- Twitter API\n",
    "- Reading Data from DataBase db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import bs4\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='font-size:20px'>\n",
    "Suppose that we wish to build a system that warns us when stock prices starts to go up or go down. The first thing we want to do is to scrape a website to get the data. <br>\n",
    "Specifically, get data from website https://finance.yahoo.com/most-active\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://finance.yahoo.com/most-active'\n",
    "res = requests.get(url=url)\n",
    "soup = bs4.BeautifulSoup(res.text, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('page.html','a') as f:\n",
    "    f.write(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(soup.prettify())\n",
    "tabl = soup.select_one('#scr-res-table > div.Ovx\\(a\\).Ovx\\(h\\)--print.Ovy\\(h\\).W\\(100\\%\\) > table') ## Copied using Chrome Extension #scr-res-table > div.Ovx\\(a\\).Ovx\\(h\\)--print.Ovy\\(h\\).W\\(100\\%\\) > table\n",
    "# #scr-res-table > div.Ovx\\(a\\).Ovx\\(h\\)--print.Ovy\\(h\\).W\\(100\\%\\) > table\n",
    "\n",
    "thead = tabl.find('thead')\n",
    "tbody = tabl.find('tbody')\n",
    "\n",
    "head_lst = [hd.text for hd in thead.find_all('th')]\n",
    "\n",
    "tab_lst = list()\n",
    "for tr in tbody.find_all('tr'):\n",
    "    tr_lst = list()\n",
    "    for td in tr.find_all('td'):\n",
    "        tr_lst.append(td.text)\n",
    "    tab_lst.append(tr_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stock = pd.DataFrame(tab_lst, columns=head_lst)\n",
    "df_stock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stock['Change'] = df_stock['Change'].astype(float)\n",
    "df_stock['% Change'] = df_stock['% Change'].str.rstrip('%').astype('float') / 100.0\n",
    "df_stock['Volume'] = df_stock['Volume'].str.rstrip('M').astype(float)\n",
    "df_stock['Avg Vol (3 month)'] = df_stock['Avg Vol (3 month)'].str.rstrip('M').astype(float)\n",
    "df_stock['Price (Intraday)'] = df_stock['Price (Intraday)'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stock.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stock[df_stock['Change'] == df_stock['Change'].max()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Scrape the Table from Cancer Statistics\n",
    "Go to the site https://www.cdc.gov/cancer/kinds.htm\n",
    "Scrape the explaination for each cancer in the first cards, store it in a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.cdc.gov/cancer/kinds.htm'\n",
    "res = requests.get(url=url)\n",
    "soup = bs4.BeautifulSoup(res.text, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamic Loading and Writing CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stock.to_csv('stocks.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('stocks.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = csv.reader(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "header = next(reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "for rw in reader:\n",
    "    rows.append(rw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(rows, columns=header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('stocks.csv', 'r') as f:\n",
    "    reader = csv.reader(f)\n",
    "    header = next(reader)\n",
    "    rows = []\n",
    "    for rw in reader:\n",
    "        rows.append(rw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(rows, columns=header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_yahoo():\n",
    "    url = 'https://finance.yahoo.com/most-active'\n",
    "    res = requests.get(url=url)\n",
    "    soup = bs4.BeautifulSoup(res.text, \"html.parser\")\n",
    "\n",
    "    tabl = soup.select_one('#scr-res-table > div.Ovx\\(a\\).Ovx\\(h\\)--print.Ovy\\(h\\).W\\(100\\%\\) > table') ## Copied using Chrome Extension #scr-res-table > div.Ovx\\(a\\).Ovx\\(h\\)--print.Ovy\\(h\\).W\\(100\\%\\) > table\n",
    "    # #scr-res-table > div.Ovx\\(a\\).Ovx\\(h\\)--print.Ovy\\(h\\).W\\(100\\%\\) > table\n",
    "    \n",
    "    thead = tabl.find('thead')\n",
    "    tbody = tabl.find('tbody')\n",
    "    \n",
    "    head_lst = [hd.text for hd in thead.find_all('th')]\n",
    "    \n",
    "    tab_lst = list()\n",
    "    for tr in tbody.find_all('tr'):\n",
    "        tr_lst = list()\n",
    "        for td in tr.find_all('td'):\n",
    "            tr_lst.append(td.text)\n",
    "        tab_lst.append(tr_lst)\n",
    "\n",
    "    return pd.DataFrame(tab_lst, columns=head_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stock = scrape_yahoo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stockdic = df_stock.transpose().to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "header = df_stock.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('stocks_appd.csv','a+') as fa:\n",
    "    csvwriter = csv.DictWriter(fa, fieldnames=header)\n",
    "    csvwriter.writeheader()\n",
    "    for key, dic_i in stockdic.items():\n",
    "        csvwriter.writerow(dic_i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wikipedia Game"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Snscrape\n",
    "Snscrape provides many useful attributes available through snscrape Twitter object [(Beck, 2020)](https://betterprogramming.pub/how-to-scrape-tweets-with-snscrape-90124ed006af). You might need some of the above attributes when scraping data for your group projects.\n",
    "* url: Permalink pointing to tweet location\n",
    "* date: The date on which the tweet was created\n",
    "* content: The text content of the tweet\n",
    "* id: The id of the tweet\n",
    "* user: User object containing the following data: username, displayname, id, description, descriptionUrls, verified, created, followersCount, friendsCount, statusesCount, favouritesCount, listedCount, location, protected, linkUrl, profileImageUrl, profileBannerUrl\n",
    "* replyCount: The count of replies\n",
    "* retweetCount: The count of retweets\n",
    "* likeCount: The count of likes\n",
    "* quoteCount: The count of users that quoted the tweet and replied\n",
    "* conversationId: Appears to be the same as tweet id\n",
    "* lang: Machine generated, assumed language of the tweet\n",
    "* source: Where tweet was posted from (e.g., iPhone, Andriod, etc.)\n",
    "* retweetedTweet: The id of the original tweet (if it is a retweet)\n",
    "* mentionedUsers: The user objects of any mentioned user in the tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import snscrape.modules.twitter as sntwitter\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_list2 = []\n",
    "\n",
    "# use TwitterSearchScraper to scrape data and append tweets to list\n",
    "for i, tweet in enumerate(sntwitter.TwitterSearchScraper('let\\'s go duke since:2021-11-12 until:2021-11-13').get_items()):\n",
    "    tweets_list2.append([tweet.date, tweet.content, tweet.user.location])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df2 = pd.DataFrame(tweets_list2, columns=['Date', 'Content', 'User\\'s location'])\n",
    "tweets_df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Scrape twitter related to cancer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('geo_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d131b0ea759db0f5632b0eb4c6c7b68c9bd75391f4012810998256aaed8f7a6d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
